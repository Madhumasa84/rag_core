# ðŸ“„ PDF Semantic Search (Minimal RAG Core)

## ðŸ”Ž Overview

This project implements a **minimal semantic search pipeline** over a PDF document.

Instead of performing keyword-based search, the system:

- Converts text into dense vector representations (embeddings)
- Stores them in a vector database (ChromaDB)
- Retrieves relevant chunks using cosine similarity

This demonstrates the core retrieval mechanism used in **Retrieval-Augmented Generation (RAG)** systems.

---

## ðŸ— Architecture

### Pipeline Steps

1. Extract raw text from PDF  
2. Split text into overlapping chunks  
3. Convert each chunk into a 384-dimensional embedding (`all-MiniLM-L6-v2`)  
4. Store embeddings in ChromaDB (persistent storage)  
5. Embed the user query using the same model  
6. Perform cosine similarity search to retrieve Top-K relevant chunks  

---

### ðŸ” Flow Diagram

PDF
â†“
Chunking
â†“
Embedding (MiniLM)
â†“
Chroma Vector DB
â†“
Query Embedding
â†“
Cosine Similarity Search
â†“
Top-K Results


---

## âš™ï¸ How It Works Internally

- Each text chunk is converted into a fixed-size vector (384 dimensions).
- Semantically similar chunks produce vectors that point in similar directions.
- Cosine similarity measures the angle between vectors.
- The query is embedded into the same vector space.
- The closest vectors (Top-K) are returned as results.

> Note: No training occurs during this process. The embedding model is pre-trained and used only for inference.

---

## ðŸ›  Tech Stack

- Python  
- sentence-transformers (`all-MiniLM-L6-v2`)  
- ChromaDB  
- PyPDF  

---

## ðŸš€ Installation

```bash
git clone <repo_url>
cd rag_ini
python -m venv venv
venv\Scripts\activate
pip install -r requirements.txt

Usage

Step 1: Store Embeddings
python src/store.py
Step 2: Query the System
python src/query.py


ðŸ“šKey Learnings

Importance of chunk size and overlap in preserving semantic context

Need to use the same embedding model for documents and queries

Difference between in-memory and persistent vector databases

Cosine similarity measures vector direction rather than magnitude


ðŸ”®Future Improvements

Add LLM to generate answers from retrieved chunks (Full RAG system)

Implement sentence-aware chunking

Add metadata filtering

Build CLI interface for dynamic querying

ðŸ“‚Repository Structure
rag_ini/
â”‚
â”œâ”€â”€ data/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ load_pdf.py
â”‚   â”œâ”€â”€ chunk.py
â”‚   â”œâ”€â”€ embed.py
â”‚   â”œâ”€â”€ store.py
â”‚   â””â”€â”€ query.py
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md